import tensorflow as tf
from tensorflow.keras.layers import Input, Embedding, Dense, Concatenate, Flatten
from tensorflow.keras.models import Model
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import numpy as np
import pandas as pd

# Define function to create the neural network model
def create_nn_model(vocab_size: int, embedding_dim: int, max_length: int, num_of_dense: int, output_dim: int) -> Model:
    """
    Creates a neural network model that processes user prompts using an embedding layer,
    concatenates it with function ratings, and passes through dense layers.

    Args:
        vocab_size (int): Size of the vocabulary for embedding.
        embedding_dim (int): Dimensionality of the embedding layer.
        max_length (int): Maximum length of input sequences.
        num_of_dense (int): Number of dense layers before concatenation.
        output_dim (int): Dimension of output layer

    Returns:
        Model: A compiled TensorFlow model.
    """
    # Text input (user prompt)
    text_input = Input(shape=(max_length,), name="text_input")
    embedding = Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length)(text_input)
    flatten = Flatten()(embedding)

    # Dense layers for text input
    num_neurons = 2**12 # Start with 4096 neurons
    x = flatten
    for _ in range(num_of_dense):
        num_neurons = max(1, int(num_neurons / 2)) # Ensure integer neurons, minimum of 1
        x = Dense(num_neurons, activation='relu')(x)

    # Numeric input (func_rating)
    func_rating_input = Input(shape=(1,), name="func_rating_input")
    y = Dense(32, activation='relu')(func_rating_input)

    # Concatenate both paths
    concatenated = Concatenate()([x, y])
    # output = Dense(1, activation='linear', name="output")(concatenated) # Regression, not applicable in the notebook's context
    output = Dense(output_dim, activation='softmax', name="output")(concatenated)  #Output dimension is the length of labels

    # Define and compile the model
    model = Model(inputs=[text_input, func_rating_input], outputs=output)

    # Compile
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

    return model

# Example usage:
urls = [
    "https://github.com/vrutika-prajapati/Credibility-Score-for-Articles/blob/main/projects/deliverable%202/Url_validation.csv",
    "https://github.com/Saikumar08-sk/URL-Validation/blob/main/Deliverable.csv",
    "https://github.com/aditya19111/Project-1-Credibility-Score-for-Articles-Sources-References/blob/main/Deliverable_2/Deliverables_rating_comparison%20-%20Sheet1.csv",
    "https://github.com/SAIKUMAR500/algorithums-for-data-science/blob/main/deliverable2/dataset(Sheet1).csv",
    "https://github.com/drashti-sanghani/Credibility_score_articles/blob/main/Project/Deliverable2/user_ratings.csv",
    "https://github.com/anchalrai101/CREDIBILITY-SCORE/blob/main/urltesting_scores.csv",
    "https://github.com/AliSInamdar/CS676-Ali-Inamdar/blob/Project_1_Credibilty_Score/Deliverable_2/Deliverable_2.csv",
    "https://github.com/bhavnaa22/Algorithms-For-Data-Science/blob/main/Deliverable%202/Deliverable2.csv",
    "https://github.com/bhatthardik4/AlgorithmDS/blob/main/Deliverable_2/HardikBhattD2.csv",
    "https://github.com/Rakeshkanche/Algorithms-for-Data-Science/blob/main/queries_urls_ratings.csv",
    "https://github.com/kristinakim-code/Credibility-Check/blob/main/deliverable2.csv",
    "https://github.com/bhavnaa22/Algorithms-For-Data-Science/blob/main/Deliverable%202/Deliverable2.csv",
    "https://github.com/kamaldomandula/Algorithms-of-data-science-Project/blob/main/Project/project1/Deliverable2/deliverable.csv",
    "https://github.com/ChinmayShetye26/Algo-for-DS-delivery2/blob/main/Sample.csv",
    "https://github.com/krishnam229/Project1/blob/main/deliverable2/sample.csv",
    "https://github.com/drona23/Deliverable2/blob/main/output.csv",
]

import pandas as pd
import requests
from io import StringIO
from sklearn.model_selection import train_test_split
from tensorflow.keras.utils import to_categorical

def download_and_combine_csv(urls: list) -> pd.DataFrame:
    """
    Download CSV files from provided URLs and combine them row-wise.
    It will combine files if they have the expected column names or just the correct number of columns.
    Handles different encodings to avoid common decoding errors. Converts certain columns to integer form.

    Args:
        urls (list): List of strings containing the URLs of the CSV files.

    Returns:
        pd.DataFrame: A DataFrame containing the combined data from all matching CSV files.
    """
    expected_columns = {'user_prompt', 'url_to_check', 'func_rating', 'custom_rating'}
    combined_df = pd.DataFrame()

    for url in urls:
        try:
            # Modify the GitHub URL to point to the raw version of the file
            raw_url = url.replace('github.com', 'raw.githubusercontent.com').replace('/blob', '')

            response = requests.get(raw_url)
            if response.status_code == 200:
                try:
                    # Attempt to read with utf-8 encoding
                    content = response.content.decode('utf-8')
                except UnicodeDecodeError:
                    # Attempt to read with latin1 encoding
                    content = response.content.decode('latin1')

                # Read the CSV file into a DataFrame
                df = pd.read_csv(StringIO(content))

                # Check if the columns match the expected columns or if there are at least 4 columns
                if set(df.columns) == expected_columns:
                    combined_df = pd.concat([combined_df, df], ignore_index=True)
                elif len(df.columns) >= 4:
                    # Assume the first 4 columns are the required ones and ignore the original column names
                    df = df.iloc[:, :4]
                    df.columns = ['user_prompt', 'url_to_check', 'func_rating', 'custom_rating']
                    combined_df = pd.concat([combined_df, df], ignore_index=True)
                else:
                    print(f"Skipping file with insufficient columns: {url}")
            else:
                print(f"Failed to download from {url}")
        except Exception as e:
            print(f"Error processing {url}: {str(e)}")

    # Ensure func_rating and custom_rating are integers
    if not combined_df.empty:
        combined_df['func_rating'] = combined_df['func_rating'].round().astype(int)
        combined_df['custom_rating'] = combined_df['custom_rating'].round().astype(int)

    return combined_df

# Download and combine data from URLs
combined_df = download_and_combine_csv(urls)

# Display the combined DataFrame (optional)
print("Combined DataFrame:")
print(combined_df.head())

# Prepare data for the neural network

# 1. Text Tokenization
tokenizer = Tokenizer(num_words=5000)  # Limit vocabulary size to 5000
tokenizer.fit_on_texts(combined_df['user_prompt'])
sequences = tokenizer.texts_to_sequences(combined_df['user_prompt'])

# 2. Padding Sequences
max_length = 10  # Define a maximum sequence length
padded_sequences = pad_sequences(sequences, maxlen=max_length)

# 3. Numeric Feature
func_ratings = combined_df['func_rating'].values

# 4. Target Variable (custom_rating)
# Convert custom_rating to categorical data for multi-class classification
custom_ratings = combined_df['custom_rating'].values
num_classes = len(np.unique(custom_ratings)) # number of unique labels

#Check the labels
print("labels: ", np.unique(custom_ratings))

#Convert target variables into categorical variables
categorical_ratings = to_categorical(custom_ratings - min(custom_ratings), num_classes=num_classes)  # Subtract min to start from 0

# 5. Split data into training and testing
X_text_train, X_text_test, X_numeric_train, X_numeric_test, y_train, y_test = train_test_split(
    padded_sequences, func_ratings, categorical_ratings, test_size=0.2, random_state=42
)

# Model Creation and Training
vocab_size = len(tokenizer.word_index) + 1  # Add 1 for the padding token

# Define hyperparameters
embedding_dim = 128
num_of_dense = 2
output_dim = num_classes # set the output dimension

# Create the model
model = create_nn_model(vocab_size, embedding_dim, max_length, num_of_dense, output_dim)

# Print model summary
model.summary()

# Train the model
epochs = 10
batch_size = 32

model.fit(
    [X_text_train, X_numeric_train],  # Input data is a list: [text_input, func_rating_input]
    y_train,
    epochs=epochs,
    batch_size=batch_size,
    validation_split=0.1  # Use a fraction of the training data for validation
)

# Evaluate the model
loss, accuracy = model.evaluate([X_text_test, X_numeric_test], y_test, verbose=0)
print('Test Accuracy: %.2f' % (accuracy*100))
